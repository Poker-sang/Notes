# 机器学习

## 误差、损失、代价函数

> $N$：样本数，遍历变量为$i$
>
> $M$：样本维度，遍历变量为$j$
>
> $\mathcal{M}\left(x\right)_{M,N}$：输入数据集
>
> $\mathcal{M}\left(y\right)_{1,N}$：实际输出数据集
>
> $\mathcal{M}\left(\hat y\right)_{1,N}$：预测输出数据集
>
> * 函数模型为线性时
>
> $\mathcal{M}\left(\theta\right)_{1,M}$：每维的权重
>
> $$\begin{aligned}
\mathcal{M}\left(\hat y\right)&=\mathcal{M}\left(x\right)\mathcal{M}\left(\theta\right)\\
\begin{bmatrix}
\hat y_1\\\hat y_2\\\vdots\\\hat y_N
\end{bmatrix}&=\begin{bmatrix}
x_{11}&x_{12}&\cdots&x_{1M}\\
x_{21}&x_{22}&\cdots&x_{2M}\\
\vdots&\vdots&\ddots&\vdots\\
x_{N1}&x_{N2}&\cdots&x_{NM}
\end{bmatrix}\begin{bmatrix}
\theta_1\\\theta_2\\\vdots\\\theta_M
\end{bmatrix}
\end{aligned}$$

### 回归函数

$$\def \L {&&L\left(\theta\right)&=}
\begin{aligned}
MSE（均方误差）:\L\frac1N\sum_i\left(\hat y_i-y_i\right)^2\\
\end{aligned}$$

### 分类函数（$y$取离散值）

$$\def \L {&&L\left(\theta\right)&=}
\begin{aligned}
二分类y_i\in\{0,1\}:\\
NLL（负对数似然）:\L-\frac1N\sum_i\left[y_i\ln\hat y_i+\left(1-y_i\right)\ln\left(1-\hat y_i\right)\right]\\
Hinge:\L\frac1N\sum_i\max\{0,1-\hat y_i\cdot y_i\}\\

多分类:\\
交叉熵:\L\frac1K\sum_i\sum_ky_{i,k}\cdot\ln\hat y_{i,k}&&y_{i,k}\in\{0,1\}\\
指数:\L\sum_i{\rm e}^{-\hat y_i\cdot y_i}
\end{aligned}$$

## 激活函数

> 类似广义线性回归中链接函数的反函数

$$\begin{aligned}
Sigmoid:&\\
Logistic:&&L\left(x\right)&=\frac1{1+{\rm e}^{-x}}\\
Tanh:&&f\left(x\right)&=\tanh\left(x\right)\\
Other:&\\
ReLU:&&\sigma\left(x\right)&=
\left\{\begin{aligned}
&x&&x>0\\
&0&&x\leqslant0
\end{aligned}\right.
\end{aligned}$$

## 目标函数求法

> $$\min_\theta\{L\left(\theta\right)\}$$

### 梯度下降（$\eta$：学习率）

$$\theta_j:=\theta_j-\eta\frac{\partial L\left(\theta\right)}{\partial\theta_j}$$

### 正则化

#### 线性回归 和 $Logistic$

$$L^*\left(\theta\right)=L\left(\theta\right)+\lambda^*\sum_j\theta^2_j$$

$$\begin{aligned}
\theta_j:&=\theta_j-\eta\frac{\partial L^*\left(\theta\right)}{\partial\theta_j}\\
&=\left(1-2\eta\lambda^*\right)\theta_j-\eta\frac{\partial L\left(\theta\right)}{\partial\theta_j}\\
&\overset{\lambda=2\eta\lambda^*}\implies\left(1-\lambda\right)\theta_j-\eta\frac{\partial L\left(\theta\right)}{\partial\theta_j}
\end{aligned}$$

### 反向传播

$$\begin{aligned}
\delta_j^{\left(L\right)}&=\theta_j^{\left(L\right)}\delta^{\left(L+1\right)}\cdot g^\prime z_j^{\left(L\right)}\\
\delta^{\left(Last\right)}&=a^{\left(Last\right)}-y
\end{aligned}$$
